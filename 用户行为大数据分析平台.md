# 网站用户行为大数据分析平台

## 项目描述

本项目是面向企业业务的数据产品，采用Spark技术开发的大数据分析平台，对网站的用户行为进行复杂的分析。用统计分析出来的数据，分析现有产品的情况，并根据用户行为分析结果改进产品的设计、调整公司的战略和业务，最终达到用大数据技术来帮助提升公司的业绩。



## 项目架构

HDFS + YARN + Hive + Spark + Zookeeper + Kafka + MySQL



## 项目职责

1. 使用 Spark Core 对用户访问session数据进行聚合统计
2. 计算关键页面单跳转转化率
3. 使用 Spark SQL 统计出各区域top3热门商品
4. 使用 Spark Streaming 实时统计广告流量



## 项目模块

### 一、用户访问session离线分析模块

#### 介绍

该模块主要是对用户访问session进行统计分析，包括session聚合指标的计；按时间比例随机抽取session；获取点击、下单、支付次数排名前10的品类；获取top10品类点击量排名前10的session。主要使用spark core实现

#### 需求

1. 按条件筛选session
2. 统计出符合条件的session中，访问时长在1s~3s、4s~6s、7s~9s、10s~30s、30s~60s、1m~3m、3m~10m、10m~30m、30m以上各个范围内的session占比；访问步长在1~3、4~6、7~9、10~30、30~60、60以上各个范围内的session占比
3. 在符合条件的session中，按照时间比例随机抽取1000个session
4. 在符合条件的session中，获取点击、下单和支付数量排名前10的品类
5. 对于排名前10的品类，分别获取其点击次数排名前10的session

#### 实现思路

1. 因为用户指定的筛选条件不同，比如按访问时间进行筛选，它是session粒度的；而针对user_info表数据有按城市、职业筛选的，筛选粒度不同；再加上数据量大的话，根据筛选的条件，就需要对全表进行扫描，这将大大影响spark的性能。针对筛选粒度的不同，统一对数据做session粒度的聚合。就是分别对user_visit_action表和user_info表的数据做聚合，然后join
2. spark是分布式，要做聚合统计逻辑，需要一个全局变量进行累加操作。spark中实现分布式累加最好的方式当然是累加器，Accumulator变量。但是问题来了，对每一个范围指标都指定一个累加器，那么累加器的数量将非常可怕，代码里充斥着这么多累加器，导致维护和修改特别复杂。所以需要自定义累加器来实现复杂的分布式计算。通过一个累加器来计算所有的指标。
3. 算法思路：求出一天的session数据量 count，求出相应时间的session数量 timecount，两者相除求出session占比，timecount/count,然后要抽取的随机session数量乘以这个比例得出要在这个时间段要抽取的数量。
4. 先计算出每个品类的点击、下单和支付的数量，使用自定义二次排序，对点击数量进行比较，相同的话再比较下单数量，再相同的话比较支付数量
5. 使用分组取topN的算法来实现，对排名前10的品类数据，按照品类id进行分组，然后求出每组品类排名前10的session

#### 实现细节

1. 通过指定的taskid，通过数据库连接池中获取连接，获取到指定taskid的任务相关数据，主要是任务参数，它是json格式。通过sql语句获取到指定时间范围的用户行为数据actionRDD，然后通过mapPartitonsToPair，映射成<sessionid,Row>的格式数据sessionid2actionRDD。再对sessionid2actionRDD做groupByKey()。由于user_info的数据是<userid,Row>格式的usreInfoRDD，所以需要对sessionid2actionRDD再做一次映射，变成<userid,partAggrInfo>格式的数据，两个做join之后，再做一次映射，变成<sessionid，fullAggrInfo>的数据。

   此时sessionid2AggrInfoRDD的格式为<sessionid,fullAggrInfo)

   fullAggrInfo的数据格式sessionid=value|searchword=value|clickcaterory=value|visitlength=value|steplength=value|starttime=value|age=value|professional=value|city=value|sex=value 

   最后根据用户指定的条件进行数据的过滤，一个或多个，年龄范围、城市、职业，session时间范围是必选的，最后使用了filter算子进行过滤

2. 自定义一个类继承AccumulatorParam，实现里面的zero、addAccumulator、addInPlace方法。在过滤数据的时候同时进行累加计算。

3. **第一步**，计算出每天每小时的session数量：

   对sessionid2AggrInfoRDD做maptopair映射，获取到获取time2pairRDD，格式为<yyyy-MM-dd_HH,aggInfo>；

   计算出每天每小时的session数量，对time2pairRDD调用countByKey算子，得出Map<String,Object>格式的数据countMap；

   **第二步**，使用时间比例抽取算法，计算出每天每小时要抽取的随机session索引：

   将<yyyy-MM-dd_HH，count>格式的countMap，转换为<yyyy-MM-dd,(HH,count)>格式的map；

   接下来按时间比例抽取对应时间段的索引值，使用random函数；

   **第三步**，遍历每天每小时的session，然后根据随机索引进行抽取：

   time2pairRDD是<yyyy-MM-dd_HH,aggrInfo>格式的RDD，执行groupByKey算子，得到<dateHour,(aggrInfo,aggInfo,...)>；

   再对其再flatMap算子

   **第四步**，获取抽取出来的session明细数据

4. **第一步**，获取所有符合条件的品类数据：

   对sessionid2detailRDD做flatMap；

   进行去重调用distinct算子，不去重的话会重复排序；

   **第二步**，计算出各品类的点击、下单、支付次数：

   调用reduceByKey算子

   **第三步**，将点击、下单、支付次数的RDD进行join：

   由于各品类的点击、下单、支付并不是所有都包含，所以使用leftOuterJoin

   **第四步**，自定义二次排序：

   定义一个类CategorySortKey实现ordered接口；

   scala有两个排序接口，ordering相当于java的comparator，ordered接口相当于java的comparable

   **第五步**，将数据影射成<CategorySortKey,info>格式的RDD，CategorySortKeyRDD，然后二次排序(降序)：

   使用map算子做映射；

   CategorySortKeyRDD调用sortByKey(false)的方法实现降序排序

5. 第一步，将热门品类top10的id生成一份RDD

   第二步，计算出各热门品类的session数量：

   对sessionid2detailRDD做groupByKey；

   调用flatMap算子，进行品类id的匹配，得到<categoryid,(sessionid,count)>格式的数据

   第三步，分组取topN算法的实现：

   算法思路：维护一个长度为10的数组，依次比较每个位置上count的数值,如果要插入的count数量比当前位置大，则当前位置及以后的数值往后移一位，count插入到当前的位置。

   ```java
   //遍历排序数据
   for (int i = 0; i < top10Sessions.length; i++) {
       //如果当前i位没有数据,那么直接将i位数据赋值为当前sessionCount
       if (top10Sessions[i] == null) {
           top10Sessions[i] = sessionCount;
           break;
       }else{
           long _count = Long.valueOf(top10Sessions[i].split(",")[1]);
   
           //如果sessionCount比i位的sessionCount要大
           if(count > _count){
               //从排序数组最后一位开始,到i位,所有数据往后挪一位
               for (int j = 9; j < i; j--) {
                   top10Sessions[j] = top10Sessions[j - 1];
               }
               //将i位赋值为sessionCount
               top10Sessions[i] = sessionCount;
               break;
           }
       }
   }
   ```

#### 技术点

1. 使用到的算子：groupByKey、countByKey、filter、join、map、flatMap、reduceByKey
2. 自定义累加器
3. 按时间比例抽取算法的实现
4. 自定义二次排序
5. 分组取topN算法实现

#### 技术难点

1. 自定义Accumulator
2. 按时间比例随机抽取算法
3. 二次排序
4. 分组取TopN

### 二、页面单跳转化率模块

#### 介绍

该模块主要计算关键页面之间单跳转转化率，涉及到页面切片算法和页面流匹配算法。

#### 需求

接受J2EE系统的taskid，通过mysql查询相应的任务，日期范围，页面id；针对指定日期范围内的用户访问数据，去判断和计算，每两个页面组成页面切片；根据指定页面流中的页面切片，计算出各个页面切片的转化率

#### 实现思路

#### 实现细节

1. 查询到指定范围的用户数据，然后做map映射，映射成<sessionid,Row>格式的数据，再对其做groupByKey()，得到session2detailsRDD<session,Iterator<>>。这样做的目的是我们不能脱离session，我们基于session对应的行为数据做页面切片

2. 页面切片生成和页面流匹配算法的实：

   拿到用户指定的页面流id 1，2，3，生成1,(1,2),(2,3)相应的页面切片；

   对session2detailsRDD做flatMap，然后按**时间顺序排序**，做相应的页面流匹配<(1,2),1>；

   最后对上一份RDD做countByKey

3. 计算出首个页面的的访问量

4. 计算转化率

#### 技术点

1. 算子：groupByKey、countByKey、flatMap
2. 页面切片生成和页面流匹配算法的实现

#### 技术难点

### 三、热门商品离线统计模块

#### 介绍

实现每天统计出各个区域top3的热门商品，主要使用spark sql实现的

#### 需求

根据用户指定的时间范围统计出各区域top3的热门商品

#### 实现思路

1. 根据指定的taskid，从数据库中查询出任务相关的参数，获取到时间范围的参数，通过spark sql从hive表中提取出指定范围的数据，过滤出商品点击行为
2. 使用spark sql从mysql中提取出城市的数据信息，将用户行为的信息和城市的信息进行join，转换为DataFrame，注册为临时表
3. 自定义udf和udaf函数，udf函数的功能是使用特定的字符分隔符将两个字段拼接起来，udaf函数的作用是组内拼接去重
4. 生成各区域热门商品点击次数的临时表以及包含完整信息的各区域热门点击次数的临时表
5. 使用spark sql内置的函数case when，给area地区打上area level标记（比如华东地区 A级，华中地区B级，东北地区C级之类的），并且使用开窗函数获取top3各区域热门商品

#### 实现细节

1. 使用Spark SQL从hive表中提取数据，通过sql语句，使用sqlContext.sql(sql)转化为DataFrame，然后通过javaRDD()方法转换为RDD，进行maptopair进行格式转化，转化为<cityid，Row>的格式
2. 异构数据源之mysql的使用，通过sqlContext.read().format("jdbc").option("数据库连接的信息").load()从mysql中提取出数据，此时是DataFrame格式的数据，转化为RDD，进行map映射，映射成<cityid,Row>的格式。然后和上述数据进行join，将RDD转化为DateFrame格式的，注册为临时表
3. 定义一个类继承UserDefinedAggregateFunction类，实现里面的initialize()、update()、merge()函数，然后将其注册
4. 在spark sql中使用自定义udaf函数和udf函数
5. 先使用开窗函数row_number() over(partition by area order by click_count)建立一个子查询，然后在外层进行过滤。

#### 技术点

1. 用spark sql从Hive表中提取数据
2. 异构数据源之spark sql从mysql中提取数据，转换为DateFrame
3. 自定义UDF和UDAF函数，以及使用
4. 窗口函数的使用

#### 技术难点

1. 自定义UDF和UDAF
2. 窗口函数的使用

窗口函数：与聚合函数一样，开窗函数也是对行集组进行聚合计算，但是它不像普通聚合函数那样每组只返回一个值，开窗函数可以为每组返回多个值

### 四、广告流量实时统计模块

#### 介绍

负责统计广告流量。实现动态黑名单机制，以及对黑名单进行过滤；实现滑动窗口对广告流量的统计；实现每个区域每个广告点击流量的实时统计；实现每个区域top3点击量的广告统计。主要使用的是spark streaming实现



数据来源于kafka，spark streaming消费kafka的数据实现动态流量的计算，与kafka的集成方式的Direct，每个batch时间间隔是5秒，数据格式是timestamp province city userid adid。

#### 需求

1. 实现实时的动态黑名单机制：将点击某个广告超过一百次的用户拉黑。基于黑名单的非法广告点击流量过滤机制
2. 每天各省各城市各广告的点击流量实时统计
3. 统计每天各省top3热门广告
4. 统计各广告最近1小时内的点击量趋势：各广告最近1小时内各分钟的点击量

#### 实现思路

1. 实时计算各batch中的每天各用户对各广告的点击次数，使用filter过滤出每天对某个广告点击超过100次的黑名单用户，并写入MySQL中，生成一份动态黑名单。使用transform操作，对每个batch RDD进行处理，都动态加载MySQL中的黑名单生成RDD，然后进行join后，过滤掉batch RDD中的黑名单用户的广告点击行为。
2. 使用updateStateByKey操作，实时计算每天各省各城市各广告的点击量，并时候更新到MySQL
3. 使用transform算子和spark sql 结合,首先以每天各省各城市各广告点击量为基础，统计出每天各省份各广告点击量，使用spark sql动态将RDD转换为DataFrame，注册为临时表。使用spark sql开窗函数row_number计算出各省top3的热门广告
4. 使用window操作，对最近一小时内滑动窗口内的数据，计算出各广告各分钟的点击量。

#### 实现细节

1. 将传过来的用户广告点击日志利用mapToPair算子进行解析成 date+userid+adid 为key的形式，计数为1，然后再利用reduceByKey算子统计出每个batch中对应每天用户对特定广告的点击次数；

   对上一步中统计出来的每天用户对特定广告的点击次数利用foreachRDD算子进行写入Mysql 持久化用户点击次数；

   使用filter算子对batch中记录，拿出date userid adid去mysql数据库中查相应的用户点击次数，过滤出点击次数大于100的；

   对filter出来的黑名单用户进行userId的整个RDD的全局去重，然后再写入Mysql中的黑名单用户表。到这里就实现了动态黑名单用户表，黑名单用户是实时增加的，基于上述计算出来的动态黑名单，在最一开始，就对每个batch中的点击行为根据动态黑名单进行过滤；

   如何过滤呢？将原始日志的RDD和黑名单RDD做映射，映射成<userid,Row>的格式，然后进行左外连接，如果join不到，说明userid不在黑名单用户。

2. 拿到过滤后的用户点击数据，利用mapToPair算子转换成key为date+province+adId的字符串，value为1L的DStream。使用updateStateByKey算子累计到当天时刻，每天每个省份每个城市的广告点击量。updateStateByKey是spark streaming特有的一种算子，在spark集群内存中，维护一份key的全局状态。

3. 使用transform算子，对<date_province_city_adid,clickCount>的数据做map映射，映射成<date_province_adid,clickCount>的格式数据，然后使用reduceByKey做统计，这样就计算出每天各省份各广告的点击量；然后将其转换为DataFrame格式的数据，注册为临时表，通过Spark sql开窗函数row_number统计出前3的热门广告

4. 传入原始的日志数据，做maptoPair的转换，转换为<yyyyMMddHHmm_adid,1L>格式的数据；对其使用reduceByKeyAndWindow操作，窗口时长为60分钟，滑动步长为10秒滑动1次；这样就拿到了最近一小时，各分钟各广告的数据

#### 技术点

1. 动态生成黑名单
2. 追踪状态变化的updateStateByKey算子的应用
3. spark streaming 滑动窗口操作

#### 技术难点

1. 动态生成黑名单
2. 追踪状态变化的updateStateByKey算子的应用
3. spark streaming 滑动窗口操作的应用



### 



